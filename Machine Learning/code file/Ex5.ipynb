{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da210db-f55c-478e-a1d9-aea6d903f636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "     --------------------------------------- 99.8/99.8 MB 19.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from xgboost) (1.10.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f89871-3f53-4832-8df9-ec050aadc538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting eli5\n",
      "  Downloading eli5-0.13.0.tar.gz (216 kB)\n",
      "     -------------------------------------- 216.2/216.2 kB 4.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: attrs>17.1.0 in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from eli5) (22.1.0)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from eli5) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from eli5) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from eli5) (1.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from eli5) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from eli5) (1.2.1)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "     ---------------------------------------- 47.0/47.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from eli5) (0.8.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from jinja2>=3.0.0->eli5) (2.1.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->eli5) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\it tools\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->eli5) (2.2.0)\n",
      "Building wheels for collected packages: eli5\n",
      "  Building wheel for eli5 (setup.py): started\n",
      "  Building wheel for eli5 (setup.py): finished with status 'done'\n",
      "  Created wheel for eli5: filename=eli5-0.13.0-py2.py3-none-any.whl size=107768 sha256=406b5f1022f79aec0d22920d3b6b356e9dcc498b44fddc95bbdfec6e74d6f0ac\n",
      "  Stored in directory: c:\\users\\minhd\\appdata\\local\\pip\\cache\\wheels\\79\\6c\\4b\\13ea3238d017bc19711b4312359e2c95be91a1a9d60d949572\n",
      "Successfully built eli5\n",
      "Installing collected packages: graphviz, eli5\n",
      "Successfully installed eli5-0.13.0 graphviz-0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1737bf1-6d1b-4491-88bf-61de02a8110a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\minhd\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#data visualization\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "#for setting up train and test sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "#for extreme gradient boosting\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#machine learning metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#model explaining\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "#result table\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f629235-fcc7-4675-85c4-5e275cca3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, Ridge, SGDRegressor, ElasticNetCV\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler,\n",
    "                                   PolynomialFeatures)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_predict, train_test_split, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import MultiTaskLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1333dcbd-5891-4d35-b311-5efd1014eea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5549505086168922"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train.csv',sep = ';')\n",
    "df.drop_duplicates()\n",
    "df = pd.get_dummies(df, columns=['type'], drop_first=True)\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "poly = PolynomialFeatures(degree = 2, include_bias = False)\n",
    "\n",
    "y = df[\"quality\"]\n",
    "x = df.drop([\"quality\"], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.transform(x_test)\n",
    "s = StandardScaler()\n",
    "x_train_s = s.fit_transform(x_train_poly)\n",
    "x_test_s = s.transform(x_test_poly)\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(x_train_s, y_train)\n",
    "# Make predictions\n",
    "predictions = model.predict(x_test_s)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c7409-97f6-4f91-acfe-9dcd2490b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c978b741-a829-4fef-83f6-2c8583410ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv', sep = ';')\n",
    "\n",
    "test_data = pd.get_dummies(test_data, columns=['type'], drop_first=True)\n",
    "\n",
    "id = test_data['id']\n",
    "test_data = test_data.drop(['id'], axis=1)\n",
    "# test_data = pd.get_dummies(test_data, columns=['type'], drop_first=True)\n",
    "# test_data = test_data.values\n",
    "# test_data = scaler.transform(test_data)\n",
    "test_data_poly = poly.transform(test_data)\n",
    "test_data_s = s.transform(test_data_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6a4d479-709f-4079-82dd-90b02f87065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_test = model.predict(test_data_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c2c69ae-18ff-46f1-8ede-6bba254c2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'id': id, 'quality': y_predict_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0944c49b-22be-47d4-b27b-6d214500c06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to results.csv\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(data)\n",
    "csv_filename = 'results.csv'\n",
    "dataframe.to_csv(csv_filename, index = False)\n",
    "print(f'DataFrame saved to {csv_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9842c-9d4c-4c43-8623-ea68f9bf7e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
